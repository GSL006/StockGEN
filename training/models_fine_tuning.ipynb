{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88753dbc",
   "metadata": {},
   "source": [
    "## Price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7ce357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.0120 - Val Loss: 0.0054\n",
      "Epoch 2/50 - Train Loss: 0.0042 - Val Loss: 0.0051\n",
      "Epoch 3/50 - Train Loss: 0.0038 - Val Loss: 0.0051\n",
      "Epoch 4/50 - Train Loss: 0.0037 - Val Loss: 0.0052\n",
      "Epoch 5/50 - Train Loss: 0.0036 - Val Loss: 0.0051\n",
      "Epoch 6/50 - Train Loss: 0.0036 - Val Loss: 0.0050\n",
      "Epoch 7/50 - Train Loss: 0.0035 - Val Loss: 0.0051\n",
      "Epoch 8/50 - Train Loss: 0.0035 - Val Loss: 0.0050\n",
      "Epoch 9/50 - Train Loss: 0.0036 - Val Loss: 0.0069\n",
      "Epoch 10/50 - Train Loss: 0.0035 - Val Loss: 0.0053\n",
      "Epoch 11/50 - Train Loss: 0.0034 - Val Loss: 0.0051\n",
      "Epoch 12/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 13/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 14/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 15/50 - Train Loss: 0.0034 - Val Loss: 0.0051\n",
      "Epoch 16/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 17/50 - Train Loss: 0.0034 - Val Loss: 0.0051\n",
      "Epoch 18/50 - Train Loss: 0.0034 - Val Loss: 0.0051\n",
      "Epoch 19/50 - Train Loss: 0.0034 - Val Loss: 0.0051\n",
      "Epoch 20/50 - Train Loss: 0.0034 - Val Loss: 0.0051\n",
      "Epoch 21/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 22/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 23/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 24/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 25/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 26/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 27/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 28/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 29/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 30/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 31/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 32/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 33/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 34/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 35/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 36/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 37/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 38/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 39/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 40/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 41/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 42/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 43/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 44/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 45/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 46/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 47/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 48/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 49/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "Epoch 50/50 - Train Loss: 0.0034 - Val Loss: 0.0050\n",
      "âœ… Model and global scaler saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 0. Fix random seeds\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 1. Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1), :]\n",
    "\n",
    "# 2. Transformer Model (7 prices + 3 sentiment probs)\n",
    "class StockGenWithSentimentProbs(nn.Module):\n",
    "    def __init__(self, input_dim=10):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, 64)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=64)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, dropout=0.3, activation='gelu'),\n",
    "            num_layers=4\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(64)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch, dim)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # (batch, seq_len, dim)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "# 3. Load and preprocess data\n",
    "csv_path = \"sentiment_data_news_2yr.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Global normalization of 'Close' prices\n",
    "scaler = StandardScaler()\n",
    "df['Close'] = scaler.fit_transform(df[['Close']])\n",
    "\n",
    "# 4. Create lag features\n",
    "for lag in range(1, 8):\n",
    "    df[f'Close_t-{lag}'] = df['Close'].shift(lag)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 5. Define features\n",
    "feature_columns = [f\"Close_t-{i}\" for i in range(1, 8)] + [\n",
    "    \"FinBERT_neutral\", \"FinBERT_positive\", \"FinBERT_negative\"\n",
    "]\n",
    "\n",
    "# 6. Windowed sequence preparation (Predict the change, not the absolute price)\n",
    "window_size = 5\n",
    "features, targets = [], []\n",
    "\n",
    "for i in range(len(df) - window_size):\n",
    "    window_df = df.iloc[i:i+window_size]\n",
    "    next_close = df.iloc[i+window_size]['Close']\n",
    "    change_in_price = next_close - df.iloc[i+window_size-1]['Close']\n",
    "\n",
    "    seq_features = [[row[col] for col in feature_columns] for _, row in window_df.iterrows()]\n",
    "    features.append(seq_features)\n",
    "    targets.append(change_in_price)\n",
    "\n",
    "X = torch.tensor(np.array(features), dtype=torch.float32)  # shape: (N, 5, 10)\n",
    "y = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "# 7. Create DataLoaders\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# 8. Model Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = StockGenWithSentimentProbs().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Use Learning Rate Scheduler to adjust learning rate during training\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for price_seq, labels in train_loader:\n",
    "        price_seq, labels = price_seq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(price_seq)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_seq, val_labels in val_loader:\n",
    "            val_seq, val_labels = val_seq.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_seq)\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# 9. Save model and global scaler\n",
    "torch.save(model.state_dict(), \"stock_model_with_probs.pth\")\n",
    "joblib.dump(scaler, \"stock_global_scaler.pkl\")\n",
    "print(\"âœ… Model and global scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b92ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MAE: 4.3061\n",
      "âœ… R-squared: -0.0123\n",
      "âœ… Accuracy: 94.85%\n",
      "\n",
      "ðŸ” Sample Predictions:\n",
      "Actual: 88.24 | Predicted: 87.74\n",
      "Actual: 88.24 | Predicted: 85.92\n",
      "Actual: 88.24 | Predicted: 85.80\n",
      "Actual: 88.24 | Predicted: 86.91\n",
      "Actual: 87.74 | Predicted: 84.36\n",
      "Actual: 87.74 | Predicted: 84.36\n",
      "Actual: 85.92 | Predicted: 84.36\n",
      "Actual: 85.80 | Predicted: 84.36\n",
      "Actual: 86.91 | Predicted: 84.36\n",
      "Actual: 84.36 | Predicted: 73.60\n",
      "\n",
      "ðŸ“ Saved as 'sentiment_data_news_ctsh_predictions.csv' with Predicted_Close column only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gagan\\AppData\\Local\\Temp\\ipykernel_17180\\652368812.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"stock_model_with_probs.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define model components\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1), :]\n",
    "\n",
    "class StockGenWithSentimentProbs(nn.Module):\n",
    "    def __init__(self, input_dim=10, num_layers=4):  # Allow num_layers to be set as a parameter\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, 64)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=64)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, dropout=0.1, activation='gelu'),\n",
    "            num_layers=num_layers  # Set number of layers dynamically\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(64)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch, dim)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "# 2. Load CSV and original unscaled Close prices\n",
    "test_data = pd.read_csv('sentiment_data_news_ctsh.csv')\n",
    "ticker = test_data['Ticker'].iloc[0]\n",
    "original_close = test_data['Close'].copy().values\n",
    "\n",
    "# 3. Load global scaler\n",
    "scaler = joblib.load(\"stock_global_scaler.pkl\")\n",
    "\n",
    "# 4. Define input feature columns\n",
    "feature_columns = [f\"Close_t-{i}\" for i in range(1, 8)] + [\n",
    "    \"FinBERT_neutral\", \"FinBERT_positive\", \"FinBERT_negative\"\n",
    "]\n",
    "\n",
    "# 5. Apply global scaling and create lag features\n",
    "test_data[\"Close\"] = scaler.transform(test_data[[\"Close\"]])\n",
    "\n",
    "for lag in range(1, 8):\n",
    "    test_data[f'Close_t-{lag}'] = test_data['Close'].shift(lag)\n",
    "\n",
    "test_data.dropna(inplace=True)\n",
    "window_size = 5\n",
    "input_sequences = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(test_data) - window_size):\n",
    "    window = test_data.iloc[i:i+window_size]\n",
    "    next_price = original_close[i + window_size]\n",
    "    seq = [[row[col] for col in feature_columns] for _, row in window.iterrows()]\n",
    "    input_sequences.append(seq)\n",
    "    targets.append(next_price)\n",
    "\n",
    "X = torch.tensor(np.array(input_sequences), dtype=torch.float32)\n",
    "\n",
    "# 6. Load trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StockGenWithSentimentProbs(input_dim=10, num_layers=4).to(device)  # Use 4 layers\n",
    "model.load_state_dict(torch.load(\"stock_model_with_probs.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 7. Predict delta_Close\n",
    "with torch.no_grad():\n",
    "    predicted_changes = model(X.to(device)).cpu().numpy()\n",
    "\n",
    "# 8. Compute predicted Close using Close_t-1\n",
    "predictions = []\n",
    "for i in range(len(predicted_changes)):\n",
    "    close_t_minus_1 = test_data.iloc[i + window_size - 1][\"Close_t-1\"]\n",
    "    close_t_minus_1_unscaled = scaler.inverse_transform([[close_t_minus_1]])[0][0]\n",
    "    predicted_price = close_t_minus_1_unscaled + predicted_changes[i]\n",
    "    predictions.append(predicted_price)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# 9. Evaluation\n",
    "mae = np.mean(np.abs(predictions - targets))\n",
    "r2 = 1 - np.sum((targets - predictions) ** 2) / np.sum((targets - np.mean(targets)) ** 2)\n",
    "accuracy = 1 - np.mean(np.abs((targets - predictions) / targets))\n",
    "\n",
    "print(f\"âœ… MAE: {mae:.4f}\")\n",
    "print(f\"âœ… R-squared: {r2:.4f}\")\n",
    "print(f\"âœ… Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 10. Sample predictions\n",
    "print(\"\\nðŸ” Sample Predictions:\")\n",
    "for i in range(10):\n",
    "    print(f\"Actual: {targets[i]:.2f} | Predicted: {predictions[i]:.2f}\")\n",
    "\n",
    "# 11. Re-read original file to preserve original columns\n",
    "original_df = pd.read_csv(\"sentiment_data_news_ctsh.csv\")\n",
    "\n",
    "# Align and add Predicted_Close column\n",
    "start_idx = len(original_df) - len(predictions)\n",
    "original_df = original_df.iloc[start_idx:].copy()\n",
    "original_df['Predicted_Close'] = predictions\n",
    "\n",
    "# 12. Add Predicted_Close to original_df and save to CSV\n",
    "original_df['Predicted_Close'] = predictions\n",
    "\n",
    "# Save to new CSV without trend classification\n",
    "original_df.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"\\nðŸ“ Saved as 'sentiment_data_news_ctsh_predictions.csv' with Predicted_Close column only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dce2b9",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6723c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trends updated in 'sentiment_data_news_ctsh_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing predictions file\n",
    "df = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Drop first row with NaN (from shift)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Set threshold percentage (e.g., 0.005 for 0.5%)\n",
    "threshold_percentage = 0  # No threshold margin\n",
    "\n",
    "# Containers for trend labels\n",
    "actual_trends = []\n",
    "predicted_trends = []\n",
    "\n",
    "# Rule-based classification\n",
    "for i in range(len(df)):\n",
    "    close_t_minus_1 = df.loc[i, 'Close_t-1']\n",
    "    actual_close = df.loc[i, 'Close']\n",
    "    predicted_close = df.loc[i, 'Predicted_Close']\n",
    "\n",
    "    delta_actual = actual_close - close_t_minus_1\n",
    "    delta_predicted = predicted_close - close_t_minus_1\n",
    "    threshold = threshold_percentage * close_t_minus_1\n",
    "\n",
    "    # Actual trend\n",
    "    if delta_actual > threshold:\n",
    "        actual_trend = \"Uptrend\"\n",
    "    elif delta_actual < -threshold:\n",
    "        actual_trend = \"Downtrend\"\n",
    "    else:\n",
    "        actual_trend = \"Sideways\"\n",
    "\n",
    "    # Predicted trend\n",
    "    if delta_predicted > threshold:\n",
    "        predicted_trend = \"Uptrend\"\n",
    "    elif delta_predicted < -threshold:\n",
    "        predicted_trend = \"Downtrend\"\n",
    "    else:\n",
    "        predicted_trend = \"Sideways\"\n",
    "\n",
    "    actual_trends.append(actual_trend)\n",
    "    predicted_trends.append(predicted_trend)\n",
    "\n",
    "# Add the results\n",
    "df['Trend'] = actual_trends\n",
    "df['Predicted_Trend'] = predicted_trends\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"âœ… Trends updated in 'sentiment_data_news_ctsh_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38065692",
   "metadata": {},
   "source": [
    "## Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb1b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trend column added using existing 'Close_t-1'.\n"
     ]
    }
   ],
   "source": [
    "# Adding trend to the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"sentiment_data_news_2yr.csv\")\n",
    "\n",
    "# Apply rule-based trend classification using existing Close_t-1\n",
    "def classify_trend(row):\n",
    "    if row['Close'] > row['Close_t-1']:\n",
    "        return \"Uptrend\"\n",
    "    elif row['Close'] < row['Close_t-1']:\n",
    "        return \"Downtrend\"\n",
    "    else:\n",
    "        return \"Sideways\"\n",
    "\n",
    "df['Trend'] = df.apply(classify_trend, axis=1)\n",
    "\n",
    "# Save it back to the same file\n",
    "df.to_csv(\"sentiment_data_news_2yr.csv\", index=False)\n",
    "print(\"âœ… Trend column added using existing 'Close_t-1'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d376ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Confidence column (based on historical trend agreement) added.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Confidence score based on how many past prices agree with the current trend\n",
    "confidence_scores = []\n",
    "\n",
    "# Iterate through rows\n",
    "for i in range(len(df)):\n",
    "    current_close = df.loc[i, \"Close\"]\n",
    "    current_trend = df.loc[i, \"Trend\"]\n",
    "    \n",
    "    match_count = 0\n",
    "    valid_days = 0\n",
    "\n",
    "    # Loop over t-1 to t-7\n",
    "    for t in range(1, 8):\n",
    "        col_name = f\"Close_t-{t}\"\n",
    "        if col_name in df.columns and pd.notna(df.loc[i, col_name]):\n",
    "            past_close = df.loc[i, col_name]\n",
    "            price_delta = current_close - past_close\n",
    "            valid_days += 1\n",
    "\n",
    "            if current_trend == \"Uptrend\" and price_delta > 0:\n",
    "                match_count += 1\n",
    "            elif current_trend == \"Downtrend\" and price_delta < 0:\n",
    "                match_count += 1\n",
    "            elif current_trend == \"Sideways\" and abs(price_delta) < 0.01:\n",
    "                match_count += 1\n",
    "\n",
    "    if valid_days > 0:\n",
    "        confidence = round((match_count / valid_days) * 100, 2)\n",
    "    else:\n",
    "        confidence = None\n",
    "\n",
    "    confidence_scores.append(confidence)\n",
    "\n",
    "# Add confidence to DataFrame\n",
    "df[\"Confidence\"] = confidence_scores\n",
    "\n",
    "# Drop any rows where confidence couldn't be computed\n",
    "df = df.dropna(subset=[\"Confidence\"]).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"âœ… Confidence column (based on historical trend agreement) added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a51b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 14.29 â†’ 4511 rows\n",
      "Confidence: 28.57 â†’ 3295 rows\n",
      "Confidence: 42.86 â†’ 2922 rows\n",
      "Confidence: 57.14 â†’ 3191 rows\n",
      "Confidence: 71.43 â†’ 3502 rows\n",
      "Confidence: 85.71 â†’ 4127 rows\n",
      "Confidence: 100.0 â†’ 17887 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"sentiment_data_news_2yr.csv\")\n",
    "\n",
    "# Round confidence values to standardize (optional, if needed)\n",
    "# df[\"Confidence\"] = df[\"Confidence\"].round(2)\n",
    "\n",
    "# Get unique values and counts\n",
    "value_counts = df[\"Confidence\"].value_counts().sort_index()\n",
    "\n",
    "# Print each unique confidence value and its count\n",
    "for confidence, count in value_counts.items():\n",
    "    print(f\"Confidence: {confidence} â†’ {count} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd07db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 1.7930\n",
      "Epoch 2 | Avg Loss: 0.9936\n",
      "Epoch 3 | Avg Loss: 0.9355\n",
      "âœ… Model fine-tuned with LoRA/PEFT and saved to './finetuned_lora_model'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Check if CUDA is available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentiment_data_news_2yr.csv\")\n",
    "\n",
    "# Optional: use a subset for faster prototyping\n",
    "# df = df.sample(1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare text and labels\n",
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trend = row[\"Trend\"]\n",
    "    close_t_1_to_7 = [row[f\"Close_t-{t}\"] for t in range(1, 8)]\n",
    "    close = row[\"Close\"]\n",
    "    confidence = row[\"Confidence\"]\n",
    "\n",
    "    input_text = (\n",
    "        f\"As a financial expert, you are tasked with predicting the confidence level of a stock trend continuing.\\n\"\n",
    "        f\"Trend: {trend}\\n\"\n",
    "        f\"Past 7-Day Closing Prices: {close_t_1_to_7}\\n\"\n",
    "        f\"Current Close: {close}\\n\"\n",
    "        f\"Please output the confidence score.\"\n",
    "    )\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    labels.append(str(confidence))  # Use string so tokenizer can handle it\n",
    "\n",
    "# Tokenizer and model (using CUDA if available)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "# Tokenize input and labels\n",
    "encodings = tokenizer(input_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "label_encodings = tokenizer(labels, padding=True, truncation=True, max_length=16, return_tensors=\"pt\")\n",
    "\n",
    "# Custom Dataset\n",
    "class ConfidenceDataset(Dataset):\n",
    "    def __init__(self, input_encodings, label_encodings):\n",
    "        self.input_ids = input_encodings[\"input_ids\"]\n",
    "        self.attention_mask = input_encodings[\"attention_mask\"]\n",
    "        self.labels = label_encodings[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }\n",
    "\n",
    "dataset = ConfidenceDataset(encodings, label_encodings)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 3\n",
    "batch_size = 4  # You can increase this if you have enough GPU memory\n",
    "lr = 5e-5\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# PEFT configuration for LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Initialize PEFT model\n",
    "model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} | Avg Loss: {avg_loss:.4f}\")\n",
    "    gc.collect()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./finetuned_lora_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_lora_model\")\n",
    "print(\"âœ… Model fine-tuned with LoRA/PEFT and saved to './finetuned_lora_model'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference complete. Predictions saved to 'sentiment_data_news_ctsh_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer and base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Load PEFT configuration and model\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"./finetuned_lora_model_confidence\")\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n",
    "# Load your new inference dataset\n",
    "df_infer = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Generate inputs (same format as training)\n",
    "input_texts = []\n",
    "for _, row in df_infer.iterrows():\n",
    "    trend = row[\"Trend\"]\n",
    "    close_t_1_to_7 = [row[f\"Close_t-{t}\"] for t in range(1, 8)]\n",
    "    close = row[\"Close\"]\n",
    "\n",
    "    input_text = (\n",
    "        f\"As a financial expert, you are tasked with predicting the confidence level of a stock trend continuing.\\n\"\n",
    "        f\"Trend: {trend}\\n\"\n",
    "        f\"Past 7-Day Closing Prices: {close_t_1_to_7}\\n\"\n",
    "        f\"Current Close: {close}\\n\"\n",
    "        f\"Please output the confidence score.\"\n",
    "    )\n",
    "    input_texts.append(input_text)\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    generated_ids = peft_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=16,\n",
    "        temperature=1,    \n",
    "        top_p=0.9,          \n",
    "        do_sample=True     \n",
    "    )\n",
    "\n",
    "    preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df_infer[\"Predicted_Confidence\"] = preds\n",
    "\n",
    "# Optional: save results\n",
    "df_infer.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"âœ… Inference complete. Predictions saved to 'sentiment_data_news_ctsh_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97405a2",
   "metadata": {},
   "source": [
    "## Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7eabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Volatility column added and saved to 'sentiment_data_news_ctsh_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Compute volatility: max(close_t-1 to t-7) - min(close_t-1 to t-7)\n",
    "def compute_volatility(row):\n",
    "    try:\n",
    "        close_prices = [row[f\"Close_t-{t}\"] for t in range(1, 8)]\n",
    "        return max(close_prices) - min(close_prices)\n",
    "    except:\n",
    "        return 0.0  # Default to 0 if any error\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"Volatility\"] = df.apply(compute_volatility, axis=1)\n",
    "\n",
    "# Save updated dataset\n",
    "df.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"âœ… Volatility column added and saved to 'sentiment_data_news_ctsh_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0f4b7",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ba4fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated short-term recommendations saved to 'sentiment_data_news_ctsh_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Define the rule-based recommendation logic for short-term (1-day) prediction\n",
    "def short_term_recommendation(actual_trend, close_prices):\n",
    "    recent_trend = close_prices[0] - close_prices[-1]  # Momentum from t-7 to t-1\n",
    "    # Rule 1: Uptrend and increasing momentum â†’ Buy\n",
    "    if actual_trend == \"Uptrend\" and recent_trend > 0:\n",
    "        return \"Buy\"\n",
    "\n",
    "    # Rule 2: Downtrend and decreasing momentum â†’ Sell\n",
    "    if actual_trend == \"Downtrend\" and recent_trend < 0:\n",
    "        return \"Sell\"\n",
    "\n",
    "    # Rule 3: Sideways trend or low momentum â†’ Hold\n",
    "    return \"Hold\"\n",
    "\n",
    "# Apply to DataFrame\n",
    "def generate_recommendations(df):\n",
    "    recs = []\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            close_prices = [row[f\"Close_t-{t}\"] for t in range(1, 8)]\n",
    "            actual_trend = row[\"Trend\"]  # Use the actual trend for recommendation\n",
    "            recs.append(short_term_recommendation(actual_trend, close_prices))\n",
    "        except Exception as e:\n",
    "            recs.append(\"Hold\")\n",
    "    return recs\n",
    "\n",
    "# Apply the logic\n",
    "df[\"Recommendation\"] = generate_recommendations(df)\n",
    "\n",
    "# Save updated DataFrame\n",
    "df.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"âœ… Updated short-term recommendations saved to 'sentiment_data_news_ctsh_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123e9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Recommendation Counts:\n",
      "Hold: 17 rows\n",
      "Buy: 2 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated DataFrame\n",
    "df = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Get counts of each recommendation\n",
    "recommendation_counts = df[\"Recommendation\"].value_counts()\n",
    "\n",
    "# Print the results\n",
    "print(\"ðŸ“Š Recommendation Counts:\")\n",
    "for label, count in recommendation_counts.items():\n",
    "    print(f\"{label}: {count} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ffdd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 0.5093\n",
      "Epoch 2 | Avg Loss: 0.2489\n",
      "Epoch 3 | Avg Loss: 0.2362\n",
      "âœ… Model fine-tuned and saved to './finetuned_recommendation_model'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"sentiment_data_news_2yr.csv\")\n",
    "\n",
    "# Create input-output pairs\n",
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trend = row[\"Trend\"]\n",
    "    confidence = row[\"Confidence\"]\n",
    "    volatility = row[\"Volatility\"]\n",
    "    close_t_1_to_7 = [row[f\"Close_t-{t}\"] for t in range(1, 8)]\n",
    "    close = row[\"Close\"]\n",
    "    recommendation = row[\"Recommendation\"]\n",
    "\n",
    "    input_text = (\n",
    "        f\"You are a financial assistant. Based on the data, make a one-day stock recommendation.\\n\"\n",
    "        f\"Trend: {trend}\\n\"\n",
    "        f\"Confidence: {confidence:.2f}\\n\"\n",
    "        f\"Volatility: {volatility:.2f}\\n\"\n",
    "        f\"Past 7-Day Closing Prices: {close_t_1_to_7}\\n\"\n",
    "        f\"Current Close: {close}\\n\"\n",
    "        f\"Recommendation (Buy, Sell, Hold):\"\n",
    "    )\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    labels.append(recommendation)\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "# Tokenize\n",
    "input_encodings = tokenizer(input_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "label_encodings = tokenizer(labels, padding=True, truncation=True, max_length=8, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, input_encodings, label_encodings):\n",
    "        self.input_ids = input_encodings[\"input_ids\"]\n",
    "        self.attention_mask = input_encodings[\"attention_mask\"]\n",
    "        self.labels = label_encodings[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "dataset = RecommendationDataset(input_encodings, label_encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} | Avg Loss: {avg_loss:.4f}\")\n",
    "    gc.collect()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./finetuned_recommendation_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_recommendation_model\")\n",
    "print(\"âœ… Model fine-tuned and saved to './finetuned_recommendation_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bdd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference complete. Predictions added to 'Predicted_Recommendation' column.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved fine-tuned model and tokenizer\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"./finetuned_recommendation_model\")\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.eval()\n",
    "\n",
    "# Load inference dataset\n",
    "df_infer = pd.read_csv(\"sentiment_data_news_ctsh_predictions.csv\")\n",
    "\n",
    "# Generate input prompts\n",
    "input_texts = []\n",
    "for _, row in df_infer.iterrows():\n",
    "    trend = row[\"Predicted_Trend\"]\n",
    "    confidence = row[\"Predicted_Confidence\"]\n",
    "    volatility = row[\"Volatility\"]\n",
    "    close_t_1_to_7 = [row[f\"Close_t-{t}\"] for t in range(1, 8)]\n",
    "    close = row[\"Predicted_Close\"]\n",
    "\n",
    "    input_text = (\n",
    "        f\"You are a financial assistant. Based on the data, make a one-day stock recommendation.\\n\"\n",
    "        f\"Trend: {trend}\\n\"\n",
    "        f\"Confidence: {confidence:.2f}\\n\"\n",
    "        f\"Volatility: {volatility:.2f}\\n\"\n",
    "        f\"Past 7-Day Closing Prices: {close_t_1_to_7}\\n\"\n",
    "        f\"Current Close: {close}\\n\"\n",
    "        f\"Recommendation (Buy, Sell, Hold):\"\n",
    "    )\n",
    "    input_texts.append(input_text)\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=8\n",
    "    )\n",
    "\n",
    "# Decode predictions\n",
    "predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df_infer[\"Predicted_Recommendation\"] = predictions\n",
    "\n",
    "# Save results to CSV\n",
    "df_infer.to_csv(\"sentiment_data_news_ctsh_predictions.csv\", index=False)\n",
    "print(\"âœ… Inference complete. Predictions added to 'Predicted_Recommendation' column.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
